# 성경말투 생성기로 바닥부터 알아보는 인공지능 글쓰기

며칠 전 [성경말투 생성기](https://mobile.twitter.com/lllimiteddd/status/1548639003380895744)라는 프로젝트를 공개했고, 지금까지 해왔던 프로젝트들중에 가장 큰 관심을 받았어요. 많은 분들이 이런 모델을 구현하는 구체적인 방법을 여쭤보셨는데, 제가 성경말투 생성기를 만든 아주 자세한 방법을 공유하려고 해요. 관련된 배경지식이 정말 하나도 없어도 쉽게 이해할 수 있도록, 인공지능의 정의부터 간결하게 작성되었습니다 ;)

다만 인공지능 구현에 대한 깊은 설명은 하지 않으려고 해요. "인공지능에 대한 배경지식은 거의 없지만, 언어모델이 어떻게 굴러가는지는 알고싶은 사람"을 대상으로 글을 작성하였습니다.

## 기술적 배경과 용어정리

성경말투 생성기는 인공지능, 그중에서도 딥러닝 기술을 활용해서 만들어졌어요. 요즘 뭐만 하면 인공지능이다, 딥러닝이다, 어디든지 붙는 마법의 단어인것 같아요. 딥러닝은 "프로그래밍으로 인간의 뇌와 비슷하게 생긴 구조를 만들어서 데이터간의 규칙을 찾아내는 문제해결 방법"을 말해요. 규칙을 올바르게 찾아가는 과정을 "학습한다"라고 불러요. 컴퓨터가 데이터에서 규칙을 스스로 찾아내게 하려면, 굉장히 많은 데이터가 필요해요. 이렇게 딥러닝을 통해 찾아낸 규칙을 "모델"이라고 불러요.

딥러닝 모델을 만들 때에는 규칙을 찾을 틀을 먼저 정해줘야 해요. Auto Encoder, SVM, RNN 등이 대표적인 틀인데, 이러한 틀들 또한 "모델"이라고 불려요. 그러니까, 우리가 원하는 딥러닝 "모델"을 만들기 위해서는 적절한 "모델"을 선택해서 방대한 양의 "데이터"를 "학습"시키면 됩니다. (여기까지 이해를 했다면, 이 글의 모든 내용을 이해할 수 있습니다😄)

## 딥러닝과 언어, 그리고 PLM

딥러닝을 통해서 풀고자 하는 문제는 여러 가지가 있어요. 특히 그중 컴퓨터로 언어를 다루는데 딥러닝을 적용하는 분야는 로켓처럼 아주 빠르게 성장하고 있어요. 하지만 우리가 쓰는 언어와 컴퓨터가 쓰는 언어는 매우 다르죠.. 컴퓨터가 쓰는 언어를 "프로그래밍 언어"라고 하는데에 반해, 우리가 사용하는 언어는 "자연어(Natural Language)"라고 불러요. 컴퓨터로 언어를 처리하는 작업을 "자연어처리(Natural Language Processing, NLP)"라고 부르고요.

위에서 딥러닝 모델을 만들 때 어마어마한 데이터가 필요하다고 했잖아요, NLP에서도 마찬가지에요. 적당한 모델을 고르고, 데이터를 잘 부어주면 돼요. 하지만 언어데이터는 너무나도 복잡해서, 일반인이나 작은 사업 규모에서 모을 수 있을 만큼의 데이터로는 최적의 규칙을 찾아내기 힘든 경우가 많아요. 혹여나 이런 데이터를 수집했다고 해도, 학습시킬만한 규모의 컴퓨터를 사용해서 모델을 끝까지 학습시킬만한 여건도 되지 않는 경우가 많고요.

가정을 하나 해서, 만약 우리가 "베트남어 댓글을 보고 별점 예측하기"라는 과제를 수행해야 한다고 해봅시다. 그렇다면 저는 서점으로 가서 베트남어 책을 먼저 살 것 같아요. 일단 베트남어에 대한 기초적인 지식을 배우고 나서 과제를 수행할 것 같아요. 내가 처리하고자 하는 언어가 어떤 문법으로 이루어져 있는지, 어떤 표현이 쓰이는지를 먼저 알아야 일도 잘 할 수 있을거니까요! 이렇게 언어에 대한 지식을 먼저 알려준 후에 구체적으로 할 일을 학습시키는 방법을 딥러닝 NLP에서도 활용하자, 학습의 효율이 훌쩍 올라가는 모습을 보였어요! 사람에게 효과적이던 학습 방법이 인공지능에게도 작용했던거에요.

여기서 한가지 발상이 등장했어요. 어차피 한 언어 안에서 쓰이는 문법과 표현은 동일하니까, 한 언어의 문법과 지식을 공통 모델로 분리해서 만들어 둔 뒤에 재활용할 수는 없을까요? 이러한 발상에서 나온게 "사전 학습된 언어 모델(Pretrained Language Model, PLM)"이에요. PLM은 어마어마한 문장들과 어마어마한 컴퓨팅 자원으로 학습된 딥러닝 모델(학습 결과물)이에요. PLM 모델에는 언어의 풍부한 지식이 녹아들어 있지만, 그 자체로는 할 수 있는 일이 제한적이에요. 그렇지만 PLM에 학습시키고 싶은 데이터셋을 넣으면, 비교적 적은 데이터로도 놀라운 성능의 NLP 모델을 만들 수 있습니다. PLM이 가지고 있는 풍부한 언어 지식 덕분에요!

PLM은 학습 방법에 따라 크게 독해를 잘하는 "이해형"과 글을 잘쓰는 "생성형"으로 나뉘어요 (이 기준은 범용적으로 쓰이는 분류는 아니지만, 이해를 돕기 위해 임의로 구분하였습니다). 자연어처리로 널리 알려져 있는 BERT가 이해형 PLM에 속해요. 문장에서 감정을 분석하고, 카테고리를 분류하고, 욕설을 감지하고, 품사를 찾아내는 등 언어 이해에 관련한 범용적인 역할에 쓰이는게 이해형 PLM이에요. 생성형 PLM에는 최근 크게 이슈가 되고 있는 GPT, HyperCLOVA등이 있어요. 우리가 만들고자 하는 성경말투 생성기도 생성형에 속해요.

생성형 PLM은 다시 두 가지 분류로 나눌 수 있어요. 원본 글이 들어가서 변형된 글이 나오는 Seq2seq(Sequence to Sequence) 모델과, 글의 앞부분을 입력하면 뒷부분을 완성시키는 모델이 있어요. 성경말투 생성기는 텍스트 스타일을 변환하는 모델이여서, 문장이 들어가서 문장이 나오는 Seq2seq 모델에 더 적합해요. 현재 공개되어 있는 한국어 Seq2seq PLM은 구글에서 만든 T5를 기반으로 KETI에서 한국어로 학습시킨 KeT5, 페이스북에서 만든 BART를 기반으로 SKT에서 한국어로 학습시킨 KoBART가 있어요. 그중 우리는 활용 예제가 풍부하고 (경험상) 비교적 적은 데이터로도 학습이 잘 되는 KoBART를 사용해보려 합니다.

## 데이터셋 준비하기

> 우리가 원하는 딥러닝 "모델"을 만들기 위해서는 적절한 "모델"을 선택해서 방대한 양의 "데이터"를 "학습"시키면 됩니다

"성경말투 생성기"를 만들기 위해 "KoBART" 모델을 선택했고, 이젠 데이터만 있으면 되겠네요! 다행히도 PLM이 있기 때문에 방대할 필요까지는 없어보여요. Seq2seq 언어모델을 학습시킬 때는 입력문장과 출력문장이 올바르게 짝지어진 데이터가 필요해요.

```
내겐 오직 너 하나야 => 나에겐 오직 너 하나뿐이라
...
```

이러한 문장이 적어도 몇천개는 있어야 해요. 어디서 데이터를 얻을 수 있을까 고민해보았고, 성경에는 여러가지의 판본이 있다는 것을 생각해 냈습니다. 우리가 흔히 말하는 "성경말투"는 "개역한글판" 번역본이에요. 아주 오래 전에 번역된 판본이라서, 그 당시의 언어 양상이 잘 드러나요. 반면 이해하기 쉽도록 쓰인 쉬운성경 판본도 있어요. 쉬운성경과 개역한글판 성경 모두 담고 있는 내용은 동일하지만, 표현하는 언어가 달라요. 그렇기 때문에, 두 판본에서 같은 뜻을 나타내는 문장을 연결해서 데이터셋을 만들 수 있어요

각 성경에서 다음과 같은 전처리를 거쳤어요
- 인용부호를 유니코드 따옴표로 변환
- 쉬운성경의 상황설명 괄호를 제거

[성경말투생성기 학습에 사용된 데이터셋입니다](https://www.kaggle.com/datasets/rycont/korean-bible-classic-modern-pair)

## BART 코드 이해하기


